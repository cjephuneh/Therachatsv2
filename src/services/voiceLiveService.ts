// Custom EventEmitter implementation for browser compatibility
class EventEmitter {
  private events: { [key: string]: Function[] } = {};

  on(event: string, listener: Function): void {
    if (!this.events[event]) {
      this.events[event] = [];
    }
    this.events[event].push(listener);
  }

  emit(event: string, ...args: any[]): void {
    if (this.events[event]) {
      this.events[event].forEach(listener => listener(...args));
    }
  }

  off(event: string, listener: Function): void {
    if (this.events[event]) {
      this.events[event] = this.events[event].filter(l => l !== listener);
    }
  }
}

export interface VoiceLiveConfig {
  subscriptionKey: string;
  region: string;
  language?: string;
  voice?: string;
}

export interface Voice {
  name: string;
  gender: 'Male' | 'Female' | 'Neutral';
  locale: string;
}

export class VoiceLiveService extends EventEmitter {
  public config: VoiceLiveConfig;
  private isConnected = false;
  private isListening = false;
  private isSpeaking = false;
  private isProcessing = false;
  private isPlaying = false;
  private isRecording = false;
  private mediaStream: MediaStream | null = null;
  private audioContext: AudioContext | null = null;
  private speechRecognizer: any = null;
  private speechSynthesizer: any = null;
  private conversationActive = false;
  private currentAudio: HTMLAudioElement | null = null;
  private mediaRecorder: MediaRecorder | null = null;
  private audioQueue: ArrayBuffer[] = [];
  private messageTimeout: NodeJS.Timeout | null = null;
  private hasReceivedMessage = false;

  constructor(config: VoiceLiveConfig) {
    super();
    this.config = config;
  }

  // Initialize the voice service
  async initialize(): Promise<void> {
    try {
      console.log('üöÄ Initializing Voice Live Service...');
      
      // Initialize audio
      await this.initializeAudio();
      
      // Set up WebSocket-based speech recognition
      await this.setupWebSocketRecognition();
      
      this.isConnected = true;
      this.emit('initialized');
      this.emit('connected'); // Also emit connected event
      console.log('‚úÖ Voice Live Service initialized successfully');
    } catch (error) {
      console.error('‚ùå Failed to initialize Voice Live Service:', error);
      this.emit('error', error);
      throw error;
    }
  }

  // Set up WebSocket-based speech recognition (WebSocket only)
  private async setupWebSocketRecognition(): Promise<void> {
    try {
      // Try WebSocket connection
      await this.tryWebSocketRecognition();
      console.log('‚úÖ WebSocket recognition setup successful');
    } catch (error) {
      console.error('‚ùå WebSocket setup failed:', error);
      throw error; // No fallbacks - WebSocket only
    }
  }

  // Try WebSocket-based recognition using Azure Speech Services binary protocol
  private async tryWebSocketRecognition(): Promise<void> {
    return new Promise((resolve, reject) => {
      // Get access token for Azure Speech Services
      this.getAccessToken().then(token => {
        // Use the correct Azure Speech Services WebSocket endpoint
        const wsUrl = `wss://${this.config.region}.stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1?language=${this.config.language || 'en-US'}`;
        
        console.log('üîó Attempting WebSocket connection to:', wsUrl);
        console.log('üîë Using access token:', token.substring(0, 20) + '...');
        
        // Create WebSocket connection with proper authentication
        const wsUrlWithAuth = `${wsUrl}&authorization=${encodeURIComponent(`Bearer ${token}`)}`;
        
        this.speechRecognizer = new WebSocket(wsUrlWithAuth);
        
        // Set up WebSocket event handlers
        this.speechRecognizer.onopen = () => {
          console.log('üé§ WebSocket connected for speech recognition');
          
          // Send initial configuration message using correct Azure Speech Services protocol
          const requestId = this.generateRequestId();
          const timestamp = new Date().toISOString();
          
          // Azure Speech Services header format
          const header = `Path: speech.config\r\nX-RequestId: ${requestId}\r\nX-Timestamp: ${timestamp}\r\nContent-Type: application/json\r\n\r\n`;
          const configMessage = {
            context: {
              system: {
                name: 'SpeechSDK',
                version: '1.0.0'
              }
            }
          };
          
          // Convert header and config to bytes
          const headerBytes = new TextEncoder().encode(header);
          const configBytes = new TextEncoder().encode(JSON.stringify(configMessage));
          
          // Create binary message: [2-byte header size][header][config data]
          const headerSize = headerBytes.length;
          const binaryMessage = new Uint8Array(2 + headerSize + configBytes.length);
          
          // Set header size in big-endian (2 bytes)
          binaryMessage[0] = (headerSize >> 8) & 0xFF; // High byte
          binaryMessage[1] = headerSize & 0xFF; // Low byte
          
          // Set header and config data
          binaryMessage.set(headerBytes, 2);
          binaryMessage.set(configBytes, 2 + headerSize);
          
          console.log('üì§ Sending configuration message:', {
            headerSize: headerSize,
            header: header.trim(),
            config: JSON.stringify(configMessage)
          });
          
          this.speechRecognizer.send(binaryMessage.buffer);
          
          // Send turn.start message to initiate conversation
          setTimeout(() => {
            const turnStartRequestId = this.generateRequestId();
            const turnStartTimestamp = new Date().toISOString();
            
            const turnStartHeader = `Path:turn.start\r\nX-RequestId: ${turnStartRequestId}\r\nX-Timestamp: ${turnStartTimestamp}\r\nContent-Type: application/json\r\n\r\n`;
            const turnStartMessage = {
              context: {
                serviceTag: turnStartRequestId
              }
            };
            
            const turnStartHeaderBytes = new TextEncoder().encode(turnStartHeader);
            const turnStartConfigBytes = new TextEncoder().encode(JSON.stringify(turnStartMessage));
            
            const turnStartHeaderSize = turnStartHeaderBytes.length;
            const turnStartBinaryMessage = new Uint8Array(2 + turnStartHeaderSize + turnStartConfigBytes.length);
            
            // Set header size in big-endian (2 bytes)
            turnStartBinaryMessage[0] = (turnStartHeaderSize >> 8) & 0xFF;
            turnStartBinaryMessage[1] = turnStartHeaderSize & 0xFF;
            
            // Set header and config data
            turnStartBinaryMessage.set(turnStartHeaderBytes, 2);
            turnStartBinaryMessage.set(turnStartConfigBytes, 2 + turnStartHeaderSize);
            
            console.log('üì§ Sending turn.start message:', {
              headerSize: turnStartHeaderSize,
              header: turnStartHeader.trim(),
              config: JSON.stringify(turnStartMessage)
            });
            
            this.speechRecognizer.send(turnStartBinaryMessage.buffer);
          }, 100); // Small delay to ensure config is processed first
          
          this.emit('listeningStarted');
          this.emit('connected'); // Emit connected event
          resolve(); // Resolve on successful connection
        };
        
        this.speechRecognizer.onmessage = (event) => {
          try {
            this.hasReceivedMessage = true; // Mark that we've received a message
            if (this.messageTimeout) {
              clearTimeout(this.messageTimeout);
              this.messageTimeout = null;
            }
            
            console.log('üì® WebSocket message received!', {
              type: typeof event.data,
              isArrayBuffer: event.data instanceof ArrayBuffer,
              isBlob: event.data instanceof Blob,
              size: event.data?.byteLength || event.data?.size || event.data?.length || 'unknown'
            });
            
            // Log the raw data for debugging
            if (event.data instanceof ArrayBuffer) {
              console.log('üì® Raw binary data:', new Uint8Array(event.data));
            } else {
              console.log('üì® Raw text data:', event.data);
            }
            
            // Handle binary messages from Azure Speech Services
            if (event.data instanceof ArrayBuffer) {
              console.log('üì® Binary message received, size:', event.data.byteLength);
              
              // Try to parse Azure Speech Services format: [MessageType][MessageLength][Data]
              try {
                const dataView = new DataView(event.data);
                
                // Check if message has proper Azure format
                if (event.data.byteLength < 5) {
                  console.log('üì® Message too short, trying direct text decode...');
                  const messageText = new TextDecoder().decode(event.data);
                  console.log('üì® Direct text message:', messageText);
                  return;
                }
                
                // Parse Azure format: [MessageType][MessageLength][Data]
                const messageType = dataView.getUint8(0);
                const messageLength = dataView.getUint32(1, true); // Little-endian
                
                console.log('üì® Azure message format:', {
                  type: messageType,
                  length: messageLength,
                  totalSize: event.data.byteLength
                });
                
                // Extract message data
                const messageBytes = new Uint8Array(event.data, 5, messageLength);
                const messageText = new TextDecoder().decode(messageBytes);
                
                console.log('üì® Message data:', messageText);
                
                // Parse as JSON
                const data = JSON.parse(messageText);
                console.log('üì® WebSocket binary message parsed:', data);
                
                if (data.RecognitionStatus === 'Success' && data.DisplayText) {
                  console.log('üé§ Speech recognized via WebSocket:', data.DisplayText);
                  this.handleUserSpeech(data.DisplayText);
                } else if (data.RecognitionStatus === 'NoMatch') {
                  console.log('üîá No speech detected');
                } else if (data.RecognitionStatus === 'InitialSilenceTimeout') {
                  console.log('‚è∞ Initial silence timeout');
                } else {
                  console.log('üì® Other recognition status:', data.RecognitionStatus);
                }
              } catch (parseError) {
                console.log('üì® Binary message is not JSON, trying direct text decode...');
                // Try to decode as text directly
                try {
                  const text = new TextDecoder().decode(event.data);
                  console.log('üì® Binary message as text:', text);
                  
                  // Try to parse as JSON
                  try {
                    const data = JSON.parse(text);
                    console.log('üì® Text message parsed as JSON:', data);
                    
                    if (data.RecognitionStatus === 'Success' && data.DisplayText) {
                      console.log('üé§ Speech recognized via WebSocket:', data.DisplayText);
                      this.handleUserSpeech(data.DisplayText);
                    }
                  } catch (jsonError) {
                    console.log('üì® Text is not JSON:', text);
                  }
                } catch (textError) {
                  console.log('üì® Binary message cannot be decoded as text');
                }
              }
            } else if (event.data instanceof Blob) {
              console.log('üì® Blob message received, size:', event.data.size);
              // Convert blob to text
              event.data.text().then(text => {
                console.log('üì® Blob as text:', text);
                try {
                  const data = JSON.parse(text);
                  console.log('üì® Blob text parsed as JSON:', data);
                  
                  if (data.RecognitionStatus === 'Success' && data.DisplayText) {
                    console.log('üé§ Speech recognized via WebSocket:', data.DisplayText);
                    this.handleUserSpeech(data.DisplayText);
                  }
                } catch (jsonError) {
                  console.log('üì® Blob text is not JSON:', text);
                }
              });
            } else {
              // Handle text messages from Azure Speech Services
              const messageText = event.data;
              console.log('üì® WebSocket text message received:', messageText);
              
              // Check if it's a turn.start message (Azure format)
              if (messageText.includes('Path:turn.start')) {
                console.log('üîÑ Turn started - waiting for speech recognition results...');
                return;
              }
              
              // Check if it's a turn.end message
              if (messageText.includes('Path:turn.end')) {
                console.log('üîÑ Turn ended');
                return;
              }
              
              // Try to parse as JSON
              try {
                // Extract JSON from Azure message format
                const jsonStart = messageText.indexOf('{');
                if (jsonStart !== -1) {
                  const jsonText = messageText.substring(jsonStart);
                  const data = JSON.parse(jsonText);
                  console.log('üì® WebSocket JSON message parsed:', data);
                  
                  if (data.RecognitionStatus === 'Success' && data.DisplayText) {
                    console.log('üé§ Speech recognized via WebSocket:', data.DisplayText);
                    this.handleUserSpeech(data.DisplayText);
                  } else if (data.RecognitionStatus === 'NoMatch') {
                    console.log('üîá No speech detected');
                  } else if (data.RecognitionStatus === 'InitialSilenceTimeout') {
                    console.log('‚è∞ Initial silence timeout');
                  } else {
                    console.log('üì® Other recognition status:', data.RecognitionStatus);
                  }
                } else {
                  console.log('üì® WebSocket text message (no JSON found):', messageText);
                }
              } catch (parseError) {
                console.log('üì® WebSocket text message parse error:', parseError);
                console.log('üì® Raw message:', messageText);
              }
            }
          } catch (error) {
            console.error('‚ùå Error parsing WebSocket message:', error);
          }
        };
        
        this.speechRecognizer.onerror = (error) => {
          console.error('‚ùå WebSocket error:', error);
          reject(new Error('WebSocket connection failed'));
        };
        
        this.speechRecognizer.onclose = (event) => {
          console.log('üõë WebSocket closed:', event.code, event.reason);
          this.emit('listeningStopped');
          this.emit('disconnected'); // Emit disconnected event
        };
        
        // Set a timeout to detect connection failure
        setTimeout(() => {
          if (this.speechRecognizer.readyState !== WebSocket.OPEN) {
            console.warn('‚ö†Ô∏è WebSocket connection timeout');
            this.speechRecognizer.close();
            reject(new Error('WebSocket connection timeout'));
          }
        }, 10000);
      }).catch(reject);
    });
  }

  // Initialize audio context and microphone access
  private async initializeAudio(): Promise<void> {
    try {
      console.log('üéµ Initializing audio...');
      
      // Request microphone access
      this.mediaStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000
        }
      });
      
      console.log('‚úÖ Microphone access granted');
      this.emit('microphoneReady');
    } catch (error) {
      console.error('‚ùå Failed to initialize audio:', error);
      throw error;
    }
  }

  // Start continuous speech-to-speech conversation (WebSocket only)
  async startConversation(): Promise<void> {
    try {
      console.log('üé§ Starting Web Speech API conversation...');
      
      // Use Web Speech API for direct browser speech recognition
      await this.setupWebSpeechAPI();
      
      this.isListening = true;
      this.conversationActive = true;
      this.emit('conversationStarted');
      console.log('‚úÖ Web Speech API conversation started successfully');
    } catch (error) {
      console.error('‚ùå Failed to start WebSocket conversation:', error);
      this.emit('error', error);
      throw error;
    }
  }

  // Stop continuous conversation
  async stopConversation(): Promise<void> {
    try {
      // Stop audio streaming
      this.stopAudioStreaming();
      
      // Close WebSocket connection
      if (this.speechRecognizer instanceof WebSocket) {
        this.speechRecognizer.close();
        this.speechRecognizer = null;
      }
      
      this.isListening = false;
      this.conversationActive = false;
      this.emit('conversationStopped');
      console.log('‚úÖ Conversation stopped');
    } catch (error) {
      console.error('‚ùå Failed to stop conversation:', error);
      this.emit('error', error);
      throw error;
    }
  }

  // Start WebSocket streaming with PCM audio format
  private async startWebSocketStreaming(): Promise<void> {
    if (!this.mediaStream) return;
    
    try {
      console.log('üé§ Starting WebSocket audio streaming...');
      
      // Create AudioContext for PCM audio processing
      this.audioContext = new AudioContext({ sampleRate: 16000 });
      const source = this.audioContext.createMediaStreamSource(this.mediaStream);
      
      // Create a ScriptProcessorNode for real-time audio processing
      const processor = this.audioContext.createScriptProcessor(4096, 1, 1);
      
      // Reset message tracking
      this.hasReceivedMessage = false;
      this.messageTimeout = null;
      
      processor.onaudioprocess = (event) => {
        if (this.speechRecognizer?.readyState === WebSocket.OPEN) {
          const inputBuffer = event.inputBuffer;
          const inputData = inputBuffer.getChannelData(0);
          
          // Convert float32 to int16 PCM
          const pcmData = new Int16Array(inputData.length);
          for (let i = 0; i < inputData.length; i++) {
            pcmData[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32768));
          }
          
          // Create binary message with correct Azure Speech Services format
          const requestId = this.generateRequestId();
          const timestamp = new Date().toISOString();
          
          // Azure Speech Services header format for audio
          const header = `Path: audio\r\nX-RequestId: ${requestId}\r\nX-Timestamp: ${timestamp}\r\nContent-Type: audio/x-wav\r\n\r\n`;
          const headerBytes = new TextEncoder().encode(header);
          
          // Create binary message: [2-byte header size][header][audio data]
          const headerSize = headerBytes.length;
          const binaryMessage = new Uint8Array(2 + headerSize + pcmData.byteLength);
          
          // Set header size in big-endian (2 bytes)
          binaryMessage[0] = (headerSize >> 8) & 0xFF; // High byte
          binaryMessage[1] = headerSize & 0xFF; // Low byte
          
          // Set header and audio data
          binaryMessage.set(headerBytes, 2);
          binaryMessage.set(new Uint8Array(pcmData.buffer), 2 + headerSize);
          
          console.log('üì§ Sending PCM audio to WebSocket, size:', binaryMessage.length);
          this.speechRecognizer.send(binaryMessage.buffer);
          
          // Set timeout to detect if we're not getting responses
          if (!this.hasReceivedMessage) {
            if (this.messageTimeout) {
              clearTimeout(this.messageTimeout);
            }
            this.messageTimeout = setTimeout(() => {
              if (!this.hasReceivedMessage) {
                console.log('‚ö†Ô∏è No WebSocket responses received, falling back to REST API');
                this.fallbackToRestAPI();
              }
            }, 5000); // Wait 5 seconds for response
          }
        }
      };
      
      // Connect the audio processing chain
      source.connect(processor);
      processor.connect(this.audioContext.destination);
      
      console.log('‚úÖ WebSocket PCM audio streaming started');
      
      // Store reference for stopping
      (this as any).audioProcessor = processor;
      
    } catch (error) {
      console.error('‚ùå Failed to start WebSocket streaming:', error);
      throw error;
    }
  }

  // Fallback to REST API when WebSocket doesn't work
  private async fallbackToRestAPI(): Promise<void> {
    try {
      console.log('üîÑ Falling back to REST API for speech recognition...');
      
      // Stop WebSocket streaming
      this.stopAudioStreaming();
      
      // Close WebSocket
      if (this.speechRecognizer instanceof WebSocket) {
        this.speechRecognizer.close();
        this.speechRecognizer = null;
      }
      
      // Set up REST API recognition
      await this.setupRestApiRecognition();
      
      // Start REST API streaming
      await this.startRestApiStreaming();
      
      console.log('‚úÖ Fallback to REST API completed');
    } catch (error) {
      console.error('‚ùå Fallback to REST API failed:', error);
      throw error;
    }
  }

  // Set up REST API-based recognition
  private async setupRestApiRecognition(): Promise<void> {
    console.log('üîÑ Setting up REST API fallback for speech recognition');
    
    try {
      // Use MediaRecorder with periodic REST API calls
      const mediaRecorder = new MediaRecorder(this.mediaStream!, {
        mimeType: 'audio/webm;codecs=opus'
      });
      
      let audioChunks: Blob[] = [];
      let isRecording = false;
      let silenceTimeout: NodeJS.Timeout | null = null;
      let lastProcessTime = 0;
      
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunks.push(event.data);
          
          // Enhanced voice activity detection
          const audioLevel = event.data.size;
          const threshold = 2000; // Adjust based on testing
          
          if (audioLevel > threshold) {
            if (!isRecording) {
              isRecording = true;
              console.log('üé§ Voice activity detected (REST API mode)');
            }
            
            // Reset silence timer
            if (silenceTimeout) {
              clearTimeout(silenceTimeout);
              silenceTimeout = null;
            }
          } else {
            if (isRecording) {
              // Set timeout to process speech after silence
              silenceTimeout = setTimeout(async () => {
                const now = Date.now();
                if (audioChunks.length > 0 && !this.isProcessing && (now - lastProcessTime) > 2000) {
                  lastProcessTime = now;
                  await this.processAudioChunks(audioChunks);
                  audioChunks = [];
                  isRecording = false;
                }
              }, 2000); // Wait 2 seconds of silence
            }
          }
        }
      };
      
      mediaRecorder.onerror = (error) => {
        console.error('‚ùå MediaRecorder error:', error);
        this.emit('error', error);
      };
      
      // Store media recorder for later use
      this.mediaRecorder = mediaRecorder;
      this.emit('listeningStarted');
      console.log('‚úÖ REST API fallback ready');
      
    } catch (error) {
      console.error('‚ùå Failed to setup REST API fallback:', error);
      throw error;
    }
  }

  // Process audio chunks using REST API
  private async processAudioChunks(audioChunks: Blob[]): Promise<void> {
    try {
      console.log('üîÑ Processing audio chunks, count:', audioChunks.length);
      
      // Combine audio chunks
      const combinedBlob = new Blob(audioChunks, { type: 'audio/webm' });
      console.log('üì¶ Combined audio blob size:', combinedBlob.size);
      
      // Send directly to REST API without PCM conversion for now
      await this.sendAudioDataDirect(combinedBlob);
      
    } catch (error) {
      console.error('‚ùå Error processing audio chunks:', error);
      this.emit('error', error);
    }
  }

  // Send audio data directly to Azure Speech-to-Text (simplified)
  private async sendAudioDataDirect(audioBlob: Blob): Promise<void> {
    try {
      console.log('üì§ Sending audio to REST API, size:', audioBlob.size);
      
      const token = await this.getAccessToken();
      
      const response = await fetch(`https://${this.config.region}.stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1?language=${this.config.language || 'en-US'}`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${token}`,
          'Content-Type': 'audio/webm; codecs=opus'
        },
        body: audioBlob
      });
      
      console.log('üì§ REST API response status:', response.status);
      
      if (!response.ok) {
        const errorText = await response.text();
        console.error('‚ùå Speech recognition failed:', response.status, errorText);
        throw new Error(`Speech recognition failed: ${response.status} - ${errorText}`);
      }
      
      const result = await response.json();
      console.log('‚úÖ Speech recognized via REST API:', result);
      
      if (result.RecognitionStatus === 'Success' && result.DisplayText) {
        console.log('üé§ Speech recognized:', result.DisplayText);
        this.handleUserSpeech(result.DisplayText);
      } else if (result.RecognitionStatus === 'Success' && result.NBest && result.NBest.length > 0) {
        // Try NBest array if DisplayText is empty
        const bestResult = result.NBest[0];
        if (bestResult.Display) {
          console.log('üé§ Speech recognized (NBest):', bestResult.Display);
          this.handleUserSpeech(bestResult.Display);
        } else {
          console.log('‚ö†Ô∏è Speech recognized but no text found:', result);
        }
      } else {
        console.log('‚ö†Ô∏è Speech recognition result:', result);
      }
      
    } catch (error) {
      console.error('‚ùå Error sending audio data:', error);
      throw error;
    }
  }

  // Convert WebM audio to PCM format
  private async convertToPCM(audioBlob: Blob): Promise<ArrayBuffer> {
    return new Promise((resolve, reject) => {
      const audio = new Audio();
      const audioContext = new AudioContext({ sampleRate: 16000 });
      
      audio.onloadeddata = () => {
        const source = audioContext.createMediaElementSource(audio);
        const processor = audioContext.createScriptProcessor(4096, 1, 1);
        
        const pcmChunks: Int16Array[] = [];
        
        processor.onaudioprocess = (event) => {
          const inputData = event.inputBuffer.getChannelData(0);
          const pcmData = new Int16Array(inputData.length);
          for (let i = 0; i < inputData.length; i++) {
            pcmData[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32768));
          }
          pcmChunks.push(pcmData);
        };
        
        audio.onended = () => {
          // Combine all PCM chunks
          const totalLength = pcmChunks.reduce((sum, chunk) => sum + chunk.length, 0);
          const combinedPCM = new Int16Array(totalLength);
          let offset = 0;
          for (const chunk of pcmChunks) {
            combinedPCM.set(chunk, offset);
            offset += chunk.length;
          }
          
          audioContext.close();
          resolve(combinedPCM.buffer);
        };
        
        source.connect(processor);
        processor.connect(audioContext.destination);
        audio.play();
      };
      
      audio.onerror = (error) => {
        audioContext.close();
        reject(error);
      };
      
      audio.src = URL.createObjectURL(audioBlob);
    });
  }

  // Send audio data to Azure Speech-to-Text
  private async sendAudioData(audioData: ArrayBuffer): Promise<void> {
    try {
      console.log('üì§ Sending audio to REST API, size:', audioData.byteLength);
      
      const token = await this.getAccessToken();
      
      const response = await fetch(`https://${this.config.region}.stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1?language=${this.config.language || 'en-US'}`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${token}`,
          'Content-Type': 'audio/wav; codecs=audio/pcm; samplerate=16000'
        },
        body: audioData
      });
      
      if (!response.ok) {
        throw new Error(`Speech recognition failed: ${response.status}`);
      }
      
      const result = await response.json();
      console.log('‚úÖ Speech recognized via REST API:', result);
      
      if (result.RecognitionStatus === 'Success' && result.DisplayText) {
        console.log('üé§ Speech recognized:', result.DisplayText);
        this.handleUserSpeech(result.DisplayText);
      } else if (result.RecognitionStatus === 'Success' && result.NBest && result.NBest.length > 0) {
        // Try NBest array if DisplayText is empty
        const bestResult = result.NBest[0];
        if (bestResult.Display) {
          console.log('üé§ Speech recognized (NBest):', bestResult.Display);
          this.handleUserSpeech(bestResult.Display);
        } else {
          console.log('‚ö†Ô∏è Speech recognized but no text found:', result);
        }
      } else {
        console.log('‚ö†Ô∏è Speech recognition result:', result);
      }
      
    } catch (error) {
      console.error('‚ùå Error sending audio data:', error);
      throw error;
    }
  }

  // Start REST API streaming
  private async startRestApiStreaming(): Promise<void> {
    if (!this.mediaRecorder) {
      throw new Error('MediaRecorder not initialized');
    }
    
    try {
      // Start the existing MediaRecorder
      this.mediaRecorder.start(100); // Send data every 100ms
      console.log('üé§ REST API streaming started');
    } catch (error) {
      console.error('‚ùå Failed to start REST API streaming:', error);
      throw error;
    }
  }

  // Stop audio streaming
  private stopAudioStreaming(): void {
    if (this.mediaRecorder) {
      this.mediaRecorder.stop();
      this.mediaRecorder = null;
    }
    
    if ((this as any).audioProcessor) {
      (this as any).audioProcessor.disconnect();
      (this as any).audioProcessor = null;
    }
    
    if (this.audioContext) {
      this.audioContext.close();
      this.audioContext = null;
    }
  }

  // Handle user speech input
  private async handleUserSpeech(text: string): Promise<void> {
    console.log('üé§ handleUserSpeech called with:', text);
    console.log('üé§ isProcessing:', this.isProcessing);
    
    if (this.isProcessing) {
      console.log('‚ö†Ô∏è Already processing, skipping...');
      return;
    }
    
    try {
      this.isProcessing = true;
      this.emit('processingStarted');
      console.log('üîÑ Processing user speech:', text);
      
      // Get AI response
      console.log('ü§ñ Getting AI response...');
      const aiResponse = await this.getAIResponse(text);
      console.log('ü§ñ AI response received:', aiResponse);
      
      // Speak the response
      console.log('üîä Speaking AI response...');
      await this.speakResponse(aiResponse);
      console.log('‚úÖ Speech response completed');
      
    } catch (error) {
      console.error('‚ùå Error processing user speech:', error);
      this.emit('error', error);
    } finally {
      this.isProcessing = false;
      this.emit('processingEnded');
      console.log('üèÅ Processing ended');
    }
  }

  // Get AI response (placeholder - replace with your AI service)
  private async getAIResponse(text: string): Promise<string> {
    console.log('ü§ñ getAIResponse called with:', text);
    
    // This is a placeholder - replace with your actual AI service
    const responses = [
      "Hello! How can I help you today?",
      "That's interesting! Tell me more.",
      "I understand. What would you like to know?",
      "Great question! Let me think about that.",
      "I'm here to help. What else can I assist you with?"
    ];
    
    const response = responses[Math.floor(Math.random() * responses.length)];
    console.log('ü§ñ Returning AI response:', response);
    return response;
  }

  // Speak response using Azure Text-to-Speech
  private async speakResponse(text: string): Promise<void> {
    try {
      console.log('üîä speakResponse called with:', text);
      this.isSpeaking = true;
      this.emit('speakingStarted');
      
      console.log('üîä Speaking response:', text);
      
      // Use Azure TTS REST API
      console.log('üéµ Synthesizing speech...');
      const audioData = await this.synthesizeSpeech(text);
      console.log('üéµ Speech synthesized, audio data size:', audioData.byteLength);
      
      // Play the audio
      console.log('üéµ Playing audio...');
      await this.playAudio(audioData);
      console.log('üéµ Audio playback completed');
      
    } catch (error) {
      console.error('‚ùå Error speaking response:', error);
      this.emit('error', error);
    } finally {
      this.isSpeaking = false;
      this.emit('speakingEnded');
      console.log('üîä Speaking ended');
    }
  }

  // Synthesize speech using Azure TTS
  private async synthesizeSpeech(text: string): Promise<ArrayBuffer> {
    console.log('üéµ synthesizeSpeech called with:', text);
    
    const token = await this.getAccessToken();
    console.log('üéµ Got access token for TTS');
    
    const voice = this.config.voice || 'en-US-AriaNeural';
    console.log('üéµ Using voice:', voice);
    
    const ssml = `
      <speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="en-US">
        <voice name="${voice}">
          ${text}
        </voice>
      </speak>
    `;
    
    console.log('üéµ SSML:', ssml);
    console.log('üéµ Making TTS request to:', `https://${this.config.region}.tts.speech.microsoft.com/cognitiveservices/v1`);
    
    const response = await fetch(`https://${this.config.region}.tts.speech.microsoft.com/cognitiveservices/v1`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${token}`,
        'Content-Type': 'application/ssml+xml',
        'X-Microsoft-OutputFormat': 'audio-16khz-128kbitrate-mono-mp3'
      },
      body: ssml
    });
    
    console.log('üéµ TTS response status:', response.status);
    console.log('üéµ TTS response headers:', Object.fromEntries(response.headers.entries()));
    
    if (!response.ok) {
      const errorText = await response.text();
      console.error('‚ùå TTS request failed:', response.status, errorText);
      throw new Error(`TTS request failed: ${response.status} - ${errorText}`);
    }
    
    const audioData = await response.arrayBuffer();
    console.log('üéµ TTS audio data received, size:', audioData.byteLength);
    
    return audioData;
  }

  // Play audio data
  private async playAudio(audioData: ArrayBuffer): Promise<void> {
    return new Promise((resolve, reject) => {
      try {
        console.log('üéµ playAudio called with audio data size:', audioData.byteLength);
        
        const blob = new Blob([audioData], { type: 'audio/mpeg' });
        const audioUrl = URL.createObjectURL(blob);
        console.log('üéµ Created audio URL:', audioUrl);
        
        const audio = new Audio(audioUrl);
        this.currentAudio = audio;
        console.log('üéµ Created audio element');
        
        audio.onended = () => {
          console.log('üéµ Audio playback ended');
          URL.revokeObjectURL(audioUrl);
          this.currentAudio = null;
          resolve();
        };
        
        audio.onerror = (error) => {
          console.error('‚ùå Audio playback error:', error);
          URL.revokeObjectURL(audioUrl);
          this.currentAudio = null;
          reject(error);
        };
        
        audio.onloadstart = () => {
          console.log('üéµ Audio loading started');
        };
        
        audio.oncanplay = () => {
          console.log('üéµ Audio can play');
        };
        
        console.log('üéµ Starting audio playback...');
        audio.play().then(() => {
          console.log('üéµ Audio play() promise resolved');
        }).catch((error) => {
          console.error('‚ùå Audio play() promise rejected:', error);
          reject(error);
        });
        
      } catch (error) {
        console.error('‚ùå Error in playAudio:', error);
        reject(error);
      }
    });
  }

  // Get access token for Azure Speech Services
  private async getAccessToken(): Promise<string> {
    const response = await fetch(`https://${this.config.region}.api.cognitive.microsoft.com/sts/v1.0/issuetoken`, {
      method: 'POST',
      headers: {
        'Ocp-Apim-Subscription-Key': this.config.subscriptionKey,
        'Content-Type': 'application/x-www-form-urlencoded'
      }
    });
    
    if (!response.ok) {
      throw new Error(`Failed to get access token: ${response.status}`);
    }
    
    const token = await response.text();
    console.log('üîë Access token obtained');
    return token;
  }

  // Get available voices
  async getVoices(): Promise<Voice[]> {
    try {
      const token = await this.getAccessToken();
      
      const response = await fetch(`https://${this.config.region}.tts.speech.microsoft.com/cognitiveservices/voices/list`, {
        headers: {
          'Authorization': `Bearer ${token}`
        }
      });
      
      if (!response.ok) {
        throw new Error(`Failed to get voices: ${response.status}`);
      }
      
      const voices = await response.json();
      return voices.map((voice: any) => ({
        name: voice.ShortName,
        gender: voice.Gender,
        locale: voice.Locale
      }));
    } catch (error) {
      console.error('‚ùå Failed to get voices:', error);
      throw error;
    }
  }

  // Test system functionality
  async testSystem(): Promise<void> {
    try {
      console.log('üß™ Testing system...');
      
      // Test WebSocket connection
      if (this.speechRecognizer instanceof WebSocket) {
        console.log('üîó WebSocket state:', this.speechRecognizer.readyState);
        console.log('üîó WebSocket URL:', this.speechRecognizer.url);
        
        if (this.speechRecognizer.readyState === WebSocket.OPEN) {
          console.log('‚úÖ WebSocket is open and ready');
          
          // Send a test message to see if we get any response
          const testMessage = {
            context: {
              system: {
                name: 'SpeechSDK',
                version: '1.0.0'
              }
            }
          };
          
          const messageBytes = new TextEncoder().encode(JSON.stringify(testMessage));
          const header = new ArrayBuffer(4);
          const headerView = new DataView(header);
          headerView.setUint32(0, messageBytes.length, false);
          
          const binaryMessage = new Uint8Array(header.byteLength + messageBytes.length);
          binaryMessage.set(new Uint8Array(header), 0);
          binaryMessage.set(messageBytes, header.byteLength);
          
          console.log('üì§ Sending test message to WebSocket...');
          this.speechRecognizer.send(binaryMessage.buffer);
        } else {
          console.log('‚ùå WebSocket is not open, state:', this.speechRecognizer.readyState);
        }
      } else {
        console.log('‚ùå No WebSocket connection');
      }
      
      // Test TTS
      const testText = "Hello! This is a test of the voice system.";
      console.log('üîä Testing TTS with:', testText);
      await this.speakResponse(testText);
      
      console.log('‚úÖ System test completed');
    } catch (error) {
      console.error('‚ùå System test failed:', error);
      throw error;
    }
  }

  // Test speech recognition manually
  async testSpeechRecognition(): Promise<void> {
    try {
      console.log('üß™ Testing speech recognition manually...');
      
      // Simulate speech recognition with a test phrase
      const testSpeech = "Hello, this is a test";
      console.log('üé§ Simulating speech recognition with:', testSpeech);
      
      // Call handleUserSpeech directly
      await this.handleUserSpeech(testSpeech);
      
      console.log('‚úÖ Speech recognition test completed');
    } catch (error) {
      console.error('‚ùå Speech recognition test failed:', error);
      throw error;
    }
  }

  // Test with actual speech recognition result
  async testWithSpeechResult(): Promise<void> {
    try {
      console.log('üß™ Testing with actual speech result...');
      
      // Simulate what Azure would return
      const mockSpeechResult = "Hello there, how are you?";
      console.log('üé§ Simulating Azure speech result:', mockSpeechResult);
      
      // Call handleUserSpeech directly
      await this.handleUserSpeech(mockSpeechResult);
      
      console.log('‚úÖ Speech result test completed');
    } catch (error) {
      console.error('‚ùå Speech result test failed:', error);
      throw error;
    }
  }

  // Test REST API speech recognition directly
  async testRestApiRecognition(): Promise<void> {
    try {
      console.log('üß™ Testing REST API speech recognition...');
      
      // Create a simple test audio blob (silence)
      const testAudio = new Blob([new ArrayBuffer(1024)], { type: 'audio/webm' });
      console.log('üì¶ Created test audio blob, size:', testAudio.size);
      
      // Try to send it to REST API
      await this.sendAudioDataDirect(testAudio);
      
      console.log('‚úÖ REST API test completed');
    } catch (error) {
      console.error('‚ùå REST API test failed:', error);
      throw error;
    }
  }

  // Generate a unique request ID (no-dash UUID format)
  private generateRequestId(): string {
    return 'xxxxxxxxxxxx4xxxyxxxxxxxxxxxxxxx'.replace(/[xy]/g, function(c) {
      const r = Math.random() * 16 | 0;
      const v = c === 'x' ? r : (r & 0x3 | 0x8);
      return v.toString(16);
    });
  }

  // Cleanup resources
  async cleanup(): Promise<void> {
    try {
      await this.stopConversation();
      
      if (this.mediaStream) {
        this.mediaStream.getTracks().forEach(track => track.stop());
        this.mediaStream = null;
      }
      
      if (this.audioContext) {
        await this.audioContext.close();
        this.audioContext = null;
      }
      
      this.isConnected = false;
      console.log('‚úÖ Cleanup completed');
    } catch (error) {
      console.error('‚ùå Cleanup failed:', error);
      throw error;
    }
  }

  // Setup Web Speech API for direct browser speech recognition
  private async setupWebSpeechAPI(): Promise<void> {
    try {
      console.log('üé§ Setting up Web Speech API...');
      
      // Check if Web Speech API is supported
      if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
        throw new Error('Web Speech API not supported in this browser');
      }
      
      // Create speech recognition instance
      const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
      this.speechRecognizer = new SpeechRecognition();
      
      // Configure speech recognition
      this.speechRecognizer.continuous = true;
      this.speechRecognizer.interimResults = false;
      this.speechRecognizer.lang = this.config.language || 'en-US';
      this.speechRecognizer.maxAlternatives = 1;
      
      // Set up event handlers
      this.speechRecognizer.onstart = () => {
        console.log('üé§ Web Speech API started');
        this.emit('listeningStarted');
      };
      
      this.speechRecognizer.onresult = (event) => {
        console.log('üé§ Web Speech API result received');
        
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const result = event.results[i];
          if (result.isFinal) {
            const transcript = result[0].transcript.trim();
            console.log('üé§ Speech recognized:', transcript);
            this.handleUserSpeech(transcript);
          }
        }
      };
      
      this.speechRecognizer.onerror = (event) => {
        console.error('‚ùå Web Speech API error:', event.error);
        this.emit('error', new Error(`Speech recognition error: ${event.error}`));
      };
      
      this.speechRecognizer.onend = () => {
        console.log('üîÑ Web Speech API ended');
        if (this.conversationActive) {
          // Restart recognition if conversation is still active
          setTimeout(() => {
            if (this.conversationActive) {
              this.speechRecognizer.start();
            }
          }, 100);
        }
      };
      
      // Start recognition
      this.speechRecognizer.start();
      
      console.log('‚úÖ Web Speech API setup complete');
      
    } catch (error) {
      console.error('‚ùå Failed to setup Web Speech API:', error);
      throw error;
    }
  }
}